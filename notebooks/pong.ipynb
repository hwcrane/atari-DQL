{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v4\")\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sample = env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(observation):\n",
    "    \"\"\"Converts the observation from 210x160 RGB numpy to 84x84 grayscale torch Tensor\"\"\"\n",
    "    transformed_obs = rgb2gray(observation)\n",
    "    transformed_obs = resize(transformed_obs, (84, 84), mode=\"constant\")\n",
    "    return torch.from_numpy(transformed_obs.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state_stack\", \"action\", \"reward\", \"next_state_stack\")\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongModel(nn.Module):\n",
    "    def __init__(self, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(), nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongAgent:\n",
    "    def __init__(self, env: gym.Env, epsilon: float) -> None:\n",
    "        self.env = env\n",
    "        self.main = PongModel(env.action_space.n).to(device)  # type: ignore\n",
    "        self.target = PongModel(env.action_space.n).to(device)  # type: ignore\n",
    "\n",
    "        self.optimiser = torch.optim.Adam(params=self.main.parameters(), lr=0.01)\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.state_stack = deque(maxlen=4)\n",
    "        self.memory = ReplayMemory(capacity=40_000)\n",
    "\n",
    "    def get_action(self, observation) -> torch.Tensor:\n",
    "        if len(self.state_stack) <= 4 or random.random() < self.epsilon:\n",
    "            return torch.tensor(\n",
    "                [[self.env.action_space.sample()]], dtype=torch.long, device=device\n",
    "            )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.argmax(self.main(observation))\n",
    "            print(x)\n",
    "            return x\n",
    "\n",
    "    def optimise(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # print(batch.state_stack[0].shape)\n",
    "\n",
    "        state_batch = torch.stack(batch.state_stack)\n",
    "        next_state_batch = torch.stack(batch.next_state_stack)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "\n",
    "        # print(state_batch.shape)\n",
    "        state_action_values = self.main(state_batch).gather(1, action_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target(next_state_batch).max(1)[0]\n",
    "\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        nn.utils.clip_grad.clip_grad_value_(self.main.parameters(), 100)\n",
    "        self.optimiser.step()\n",
    "\n",
    "    def train(self, n_episodes: int):\n",
    "        # populate state stack\n",
    "        observation, _ = self.env.reset()\n",
    "        self.state_stack.append(convert_observation(observation))\n",
    "\n",
    "        # populate state stack\n",
    "        while len(self.state_stack) < 1000:\n",
    "            action = self.get_action(self.stack_observations())\n",
    "            next_observation, _, _, _, _ = self.env.step(action)\n",
    "            self.state_stack.append(convert_observation(next_observation))\n",
    "\n",
    "        for _ in range(n_episodes):\n",
    "            observation, _ = self.env.reset()\n",
    "            self.state_stack.append(convert_observation(observation))\n",
    "            stacked_state = self.stack_observations()\n",
    "            done = False\n",
    "            step = 0\n",
    "            summed_reward = 0\n",
    "            while not done:\n",
    "                step += 1\n",
    "                # Get next action\n",
    "                action = self.get_action(self.stack_observations())\n",
    "\n",
    "                # Apply action to env\n",
    "                next_observation, reward, terminated, truncated, info = self.env.step(\n",
    "                    action\n",
    "                )\n",
    "                summed_reward += reward\n",
    "                done = terminated or truncated\n",
    "                # Push new obervation to state stack\n",
    "                self.state_stack.append(convert_observation(next_observation))\n",
    "                # Create the next state stack\n",
    "                next_stacked_state = self.stack_observations()\n",
    "\n",
    "                # Store in replay memory\n",
    "                self.memory.push(\n",
    "                    stacked_state,\n",
    "                    action,\n",
    "                    torch.tensor([reward], device=device),\n",
    "                    next_stacked_state,\n",
    "                )\n",
    "                self.optimise()\n",
    "                if step % 1000 == 0:\n",
    "                    self.target.load_state_dict(self.main.state_dict())\n",
    "            print(summed_reward)\n",
    "\n",
    "    def stack_observations(self):\n",
    "        return torch.stack(list(self.state_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.0\n",
      "-21.0\n",
      "-20.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[39m=\u001b[39m PongAgent(env, \u001b[39m0.5\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[55], line 100\u001b[0m, in \u001b[0;36mPongAgent.train\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     98\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n\u001b[1;32m     99\u001b[0m \u001b[39m# Push new obervation to state stack\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_stack\u001b[39m.\u001b[39mappend(convert_observation(next_observation))\n\u001b[1;32m    101\u001b[0m \u001b[39m# Create the next state stack\u001b[39;00m\n\u001b[1;32m    102\u001b[0m next_stacked_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_observations()\n",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m, in \u001b[0;36mconvert_observation\u001b[0;34m(observation)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts the observation from 210x160 RGB numpy to 84x84 grayscale torch Tensor\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m transformed_obs \u001b[39m=\u001b[39m rgb2gray(observation)\n\u001b[0;32m----> 4\u001b[0m transformed_obs \u001b[39m=\u001b[39m resize(transformed_obs, (\u001b[39m84\u001b[39;49m, \u001b[39m84\u001b[39;49m), mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconstant\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(transformed_obs\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/skimage/transform/_warps.py:185\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    182\u001b[0m     filtered \u001b[39m=\u001b[39m image\n\u001b[1;32m    184\u001b[0m zoom_factors \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m factors]\n\u001b[0;32m--> 185\u001b[0m out \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39;49mzoom(filtered, zoom_factors, order\u001b[39m=\u001b[39;49morder, mode\u001b[39m=\u001b[39;49mndi_mode,\n\u001b[1;32m    186\u001b[0m                cval\u001b[39m=\u001b[39;49mcval, grid_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    188\u001b[0m _clip_warp_output(image, out, mode, cval, clip)\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/ndimage/_interpolation.py:819\u001b[0m, in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[1;32m    815\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mdivide(zoom_nominator, zoom_div,\n\u001b[1;32m    816\u001b[0m                     out\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mones_like(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mfloat64),\n\u001b[1;32m    817\u001b[0m                     where\u001b[39m=\u001b[39mzoom_div \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m    818\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mascontiguousarray(zoom)\n\u001b[0;32m--> 819\u001b[0m _nd_image\u001b[39m.\u001b[39;49mzoom_shift(filtered, zoom, \u001b[39mNone\u001b[39;49;00m, output, order, mode, cval, npad,\n\u001b[1;32m    820\u001b[0m                      grid_mode)\n\u001b[1;32m    821\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = PongAgent(env, 0.5)\n",
    "agent.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21.0\n",
      "-20.0\n",
      "-21.0\n",
      "-20.0\n",
      "-21.0\n",
      "-21.0\n",
      "-20.0\n",
      "-20.0\n",
      "-21.0\n",
      "-21.0\n"
     ]
    }
   ],
   "source": [
    "agent.epsilon = 0\n",
    "\n",
    "agent.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21.0\n"
     ]
    }
   ],
   "source": [
    "agent.env = gym.make(\"Pong-v4\", render_mode=\"human\")\n",
    "agent.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
